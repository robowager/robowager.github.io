<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="stylesheet" href="/_next/static/css/0d132565a6ef74ad.css" crossorigin="" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-07206ff3622cc1e4.js" crossorigin=""/><script src="/_next/static/chunks/fd9d1056-735d320b4b8745cb.js" async="" crossorigin=""></script><script src="/_next/static/chunks/938-cd2116519108597b.js" async="" crossorigin=""></script><script src="/_next/static/chunks/main-app-0b5dd7ac61849cc9.js" async="" crossorigin=""></script><script src="/_next/static/chunks/250-e6a6f086b85a6178.js" async=""></script><script src="/_next/static/chunks/app/page-5ef23bef5d49f6f4.js" async=""></script><title>Hello Robot in VR</title><meta name="description" content="Notes of an armchair roboticist"/><script src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js" crossorigin="" noModule=""></script></head><body><article class="post"><h1>Hello Robot in VR</h1><p>2021-12-19</p><div><h2>Introduction</h2>
<p>Here's a robotics version of 'hello world' in a VR environment.</p>
<p><img src="/hello-robot-vr/recording.gif" alt="recording"></p>
<p>I'll describe how I set it up, but won't share any code. The approach was
brittle and the tools are outdated. If I had to start afresh, the high-level
approach would be to</p>
<ul>
<li>run the robotics application in ROS,</li>
<li>run the VR environment as a Unity game, with which ROS would communicate, and</li>
<li>let Unity handle the details of displaying to a headset.</li>
</ul>
<p><img src="/hello-robot-vr/1_high_level.png" alt="high-level approach"></p>
<p>I only had a <a href="https://arvr.google.com/cardboard/">Google Cardboard</a> and a wired
Xbox 360 controller. Getting a demo to work with them required a complicated
setup, shown below. I think things would be simpler with better hardware. It
was still a good learning experience, and I'll describe the role each component
played.</p>
<p><img src="/hello-robot-vr/5_overall.png" alt="overall system"></p>
<p>I should clarify that I was only viewing and not controlling the robot via VR.</p>
<h2>Components</h2>
<h3>ROS</h3>
<p>I assume some robot application of interest that exists in ROS. I used ROS1,
but ROS2 is the way to go moving forward. It is common to run ROS in a
container, and I used the <code>ros:melodic</code> base. I used the
<a href="https://github.com/personalrobotics/aikido">AIKIDO</a> library as the robot
environment, i.e. for creating a robot model from URDFs, computing forward
kinematics, and so on. The robot was a UR10 with the
<a href="https://github.com/ros-industrial/robot_movement_interface/tree/master/dependencies/ur_description">description</a>
obtained from ros-industrial. I used the classic
<a href="https://wiki.ros.org/joy">joy</a> ROS package to make use of my wired Xbox 360
controller. To summarize, I had a ROS node that</p>
<ul>
<li>created a UR10 using robot assets and AIKIDO,</li>
<li>listened to joystick commands by subscribing to a joy node,</li>
<li>updated robot joint states based on joystick commands, and</li>
<li>published robot joint states on a topic.</li>
</ul>
<p>The last step was important for the connection between ROS and Unity,
described below. ROS already has a great viewer, RViz, and AIKIDO allows easy
visualization of the robot in RViz. Some official ROS2 support for viewing RViz
in a headset (like the archived
<a href="https://github.com/ros-visualization/oculus_rviz_plugins">oculus_rviz_plugins</a>
and <a href="https://github.com/AndreGilerson/rviz_vive">rviz_vive</a> projects) would
make a project like mine unnecessary :)</p>
<h3>Unity</h3>
<p>I used Unity to build a simple game that could be viewed in VR. It had a model
of the UR10 robot that was updated by joint states published by the ROS node.
The endpoint of such an approach is to build a full robotics visualizer in
Unity, which was attempted by the
<a href="https://github.com/KIT-ISAS/iviz/tree/master/iviz">iviz</a> project.</p>
<p>I found <a href="https://github.com/Unity-Technologies/Unity-Robotics-Hub">Unity Robotics
Hub</a> very helpful for
my tasks.</p>
<ul>
<li>I used <a href="https://github.com/Unity-Technologies/Unity-Robotics-Hub/blob/main/tutorials/pick_and_place/1_urdf.md">part
1</a>
of the pick-and-place tutorial to setup the robot in Unity.  The same URDFs and
meshes used in the ROS node worked here.</li>
<li>I followed
<a href="https://github.com/Unity-Technologies/Unity-Robotics-Hub/tree/main/tutorials/ros_unity_integration">ros_unity_integration</a>
to setup comms between ROS and Unity. The ROS side config was to clone the
<a href="https://github.com/Unity-Technologies/ROS-TCP-Endpoint">ROS-TCP-Endpoint</a>
package into the workspace, build it, and run the endpoint node. On the Unity side, the
<a href="https://github.com/Unity-Technologies/ROS-TCP-Connector">ROS-TCP-Connector</a>
had to be added to the game.</li>
<li>I also had to generate the joint states message in Unity.</li>
</ul>
<p>The ROS-TCP plugin allows information to flow both ways, although I was only
subscribing to messages from ROS in Unity. It could also be argued that for
this simple demo, ROS could be eliminated by reading the wired controller
inputs directly in Unity.</p>
<p><img src="/hello-robot-vr/2_ros_tcp.png" alt="ROS-Unity communication"></p>
<p>For VR support, I used the <a href="https://github.com/ValveSoftware/steamvr_unity_plugin">SteamVR Unity
Plugin</a>.  It was easy to
use, I just had to drop the <code>CameraRig</code> prefab into the scene.  I also liked
that with Steam VR the same game could, in principle, work with different
headsets.</p>
<p><img src="/hello-robot-vr/3_unity_cardboard.png" alt="Unity-cardboard communication"></p>
<h3>Cardboard viewer</h3>
<p>I was using a Google Cardboard viewer with a Moto X4 Android phone. Connecting
the Unity game to the smartphone was cumbersome and the least disciplined
component in my system.</p>
<p>I used <a href="https://www.trinusvirtualreality.com/trinus-cardboard/">Trinus Cardboard
VR</a> to display the
Unity game on the smartphone. On the Android end, I downloaded the Trinus CBVR
Lite app.  On the PC end, I had so far been working on an Ubuntu 20.04 host.
The Unity Hub and Editor worked well for me on Ubuntu. But the provided Trinus
Cardboard VR PC server was for Windows. I did try the open source
<a href="https://github.com/MyrikLD/LinusTrinus">LinusTrinus</a> for running a Trinus
server in Linux, but it didn't work for me. I was forced to move to a Windows
host.</p>
<p><img src="/hello-robot-vr/4_unity_trinus_cardboard.png" alt="Unity-cardboard communication using Trinus"></p>
<p>Using the Unity Editor on Ubuntu, I built the Unity game to run on Windows. I
installed Docker on Windows and re-created the ROS image there. But I hit
another roadblock when trying to access the wired controller in the ROS
container. This was a <a href="https://github.com/microsoft/WSL/issues/2195">known
issue</a> on Windows. Instead of
investigating workarounds, I switched to running the ROS nodes in an Ubuntu VM
using <a href="https://www.virtualbox.org/">VirtualBox</a>. This was an unsatisfying
change, but I was close to the end. The VM ran reliably after a small amount of
configuration to pass-through the Xbox controller, and setup network so that
the ROS TCP node could communicate with the Unity game.</p>
<p><img src="/hello-robot-vr/5_overall.png" alt="overall system"></p>
<h2>Result</h2>
<p>Once all components were setup and running, I could move the robot with the
controller, and view the results on my smartphone and the Cardboard.</p>
<p><img src="/hello-robot-vr/recording.gif" alt="recording"></p>
<p>An aspect that could have used more cleanup was the launch of components. I
ended up with a fairly long list of steps to follow to run the demo, starting
with having the controlled plugged in, running processes in a specific order,
checking various IPs to ensure that comms worked, etc.</p>
<h2>Steps that didn't work</h2>
<p>A list of other things that I tried that were not part of the final system.</p>
<ul>
<li>I tried running Unity Editor in a container, just for curiosity. I couldn't
get it to work, and most of the guides online
(<a href="https://johnaustin.io/articles/2020/running-unity-20201-in-docker">example</a>)
were about running Unity in a headless mode for CI purposes. I didn't have prior
experience with Unity and wanted to try out the Editor, so continued with a
host install.</li>
<li>I thought about running the Unity game on the phone. I was able to run the
demo game from the Cardboard unity plugin
<a href="https://developers.google.com/cardboard/develop/unity/quickstart">quickstart</a>.
But Steam VR seemed like a better option.</li>
<li>I could not get the wired Xbox controller to work if plugged into the
smartphone, e.g. to play an Android game with controller support. In
addition, Unity <a href="https://docs.unity3d.com/Packages/com.unity.inputsystem@1.0/manual/SupportedDevices.html">did not
support</a>
the Xbox 360 controller on Android.</li>
<li>I could not get <a href="https://docs.unity3d.com/2020.3/Documentation/Manual/UnityRemote5.html">Unity
Remote</a>
to work when running the Unity Editor in Ubuntu. It may have had something to
do with <code>adb</code> drivers.</li>
</ul>
<h2>Final thoughts</h2>
<p>If I had time to work on one additional feature, it would be some form of
control by passing information back along VR -> Unity -> ROS. I'd have to think
of a convincing yet simple demo, e.g. using gesture to specify a desired
end-effector position, then planning and executing a motion. Effective control
with a gamepad and RViz already requires some thought, as in this
<a href="https://www.youtube.com/watch?v=p_x-HRagLpo">video</a> that
<a href="https://ros-planning.github.io/moveit_tutorials/doc/joystick_control_teleoperation/joystick_control_teleoperation_tutorial.html">demos</a>
joystick control in MoveIt.</p>
</div></article><footer><a href="/">Home</a></footer><script src="/_next/static/chunks/webpack-07206ff3622cc1e4.js" crossorigin="" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script>self.__next_f.push([1,"1:HL[\"/_next/static/css/0d132565a6ef74ad.css\",\"style\",{\"crossOrigin\":\"\"}]\n0:\"$L2\"\n"])</script><script>self.__next_f.push([1,"3:I[7690,[],\"\"]\n6:I[5613,[],\"\"]\n8:I[1778,[],\"\"]\n9:I[5250,[\"250\",\"static/chunks/250-e6a6f086b85a6178.js\",\"931\",\"static/chunks/app/page-5ef23bef5d49f6f4.js\"],\"\"]\nb:I[8955,[],\"\"]\n7:[\"slug\",\"hello-robot-vr\",\"d\"]\n"])</script><script>self.__next_f.push([1,"2:[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/0d132565a6ef74ad.css\",\"precedence\":\"next\",\"crossOrigin\":\"\"}]],[\"$\",\"$L3\",null,{\"buildId\":\"3QaHMm55SIjk_hJY0H6Bb\",\"assetPrefix\":\"\",\"initialCanonicalUrl\":\"/posts/hello-robot-vr\",\"initialTree\":[\"\",{\"children\":[\"posts\",{\"children\":[[\"slug\",\"hello-robot-vr\",\"d\"],{\"children\":[\"__PAGE__?{\\\"slug\\\":\\\"hello-robot-vr\\\"}\",{}]}]}]},\"$undefined\",\"$undefined\",true],\"initialSeedData\":[\"\",{\"children\":[\"posts\",{\"children\":[[\"slug\",\"hello-robot-vr\",\"d\"],{\"children\":[\"__PAGE__\",{},[\"$L4\",\"$L5\",null]]},[\"$\",\"$L6\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"posts\",\"children\",\"$7\",\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"loadingScripts\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"styles\":null}]]},[\"$\",\"$L6\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"posts\",\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"loadingScripts\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"styles\":null}]]},[null,[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"body\",null,{\"children\":[[\"$\",\"$L6\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"loadingScripts\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":\"404\"}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],\"notFoundStyles\":[],\"styles\":null}],[\"$\",\"footer\",null,{\"children\":[\"$\",\"$L9\",null,{\"href\":\"/\",\"children\":\"Home\"}]}]]}]}],null]],\"initialHead\":[false,\"$La\"],\"globalErrorComponent\":\"$b\"}]]\n"])</script><script>self.__next_f.push([1,"c:T22f7,"])</script><script>self.__next_f.push([1,"\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eHere's a robotics version of 'hello world' in a VR environment.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/hello-robot-vr/recording.gif\" alt=\"recording\"\u003e\u003c/p\u003e\n\u003cp\u003eI'll describe how I set it up, but won't share any code. The approach was\nbrittle and the tools are outdated. If I had to start afresh, the high-level\napproach would be to\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003erun the robotics application in ROS,\u003c/li\u003e\n\u003cli\u003erun the VR environment as a Unity game, with which ROS would communicate, and\u003c/li\u003e\n\u003cli\u003elet Unity handle the details of displaying to a headset.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/hello-robot-vr/1_high_level.png\" alt=\"high-level approach\"\u003e\u003c/p\u003e\n\u003cp\u003eI only had a \u003ca href=\"https://arvr.google.com/cardboard/\"\u003eGoogle Cardboard\u003c/a\u003e and a wired\nXbox 360 controller. Getting a demo to work with them required a complicated\nsetup, shown below. I think things would be simpler with better hardware. It\nwas still a good learning experience, and I'll describe the role each component\nplayed.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/hello-robot-vr/5_overall.png\" alt=\"overall system\"\u003e\u003c/p\u003e\n\u003cp\u003eI should clarify that I was only viewing and not controlling the robot via VR.\u003c/p\u003e\n\u003ch2\u003eComponents\u003c/h2\u003e\n\u003ch3\u003eROS\u003c/h3\u003e\n\u003cp\u003eI assume some robot application of interest that exists in ROS. I used ROS1,\nbut ROS2 is the way to go moving forward. It is common to run ROS in a\ncontainer, and I used the \u003ccode\u003eros:melodic\u003c/code\u003e base. I used the\n\u003ca href=\"https://github.com/personalrobotics/aikido\"\u003eAIKIDO\u003c/a\u003e library as the robot\nenvironment, i.e. for creating a robot model from URDFs, computing forward\nkinematics, and so on. The robot was a UR10 with the\n\u003ca href=\"https://github.com/ros-industrial/robot_movement_interface/tree/master/dependencies/ur_description\"\u003edescription\u003c/a\u003e\nobtained from ros-industrial. I used the classic\n\u003ca href=\"https://wiki.ros.org/joy\"\u003ejoy\u003c/a\u003e ROS package to make use of my wired Xbox 360\ncontroller. To summarize, I had a ROS node that\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ecreated a UR10 using robot assets and AIKIDO,\u003c/li\u003e\n\u003cli\u003elistened to joystick commands by subscribing to a joy node,\u003c/li\u003e\n\u003cli\u003eupdated robot joint states based on joystick commands, and\u003c/li\u003e\n\u003cli\u003epublished robot joint states on a topic.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe last step was important for the connection between ROS and Unity,\ndescribed below. ROS already has a great viewer, RViz, and AIKIDO allows easy\nvisualization of the robot in RViz. Some official ROS2 support for viewing RViz\nin a headset (like the archived\n\u003ca href=\"https://github.com/ros-visualization/oculus_rviz_plugins\"\u003eoculus_rviz_plugins\u003c/a\u003e\nand \u003ca href=\"https://github.com/AndreGilerson/rviz_vive\"\u003erviz_vive\u003c/a\u003e projects) would\nmake a project like mine unnecessary :)\u003c/p\u003e\n\u003ch3\u003eUnity\u003c/h3\u003e\n\u003cp\u003eI used Unity to build a simple game that could be viewed in VR. It had a model\nof the UR10 robot that was updated by joint states published by the ROS node.\nThe endpoint of such an approach is to build a full robotics visualizer in\nUnity, which was attempted by the\n\u003ca href=\"https://github.com/KIT-ISAS/iviz/tree/master/iviz\"\u003eiviz\u003c/a\u003e project.\u003c/p\u003e\n\u003cp\u003eI found \u003ca href=\"https://github.com/Unity-Technologies/Unity-Robotics-Hub\"\u003eUnity Robotics\nHub\u003c/a\u003e very helpful for\nmy tasks.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eI used \u003ca href=\"https://github.com/Unity-Technologies/Unity-Robotics-Hub/blob/main/tutorials/pick_and_place/1_urdf.md\"\u003epart\n1\u003c/a\u003e\nof the pick-and-place tutorial to setup the robot in Unity.  The same URDFs and\nmeshes used in the ROS node worked here.\u003c/li\u003e\n\u003cli\u003eI followed\n\u003ca href=\"https://github.com/Unity-Technologies/Unity-Robotics-Hub/tree/main/tutorials/ros_unity_integration\"\u003eros_unity_integration\u003c/a\u003e\nto setup comms between ROS and Unity. The ROS side config was to clone the\n\u003ca href=\"https://github.com/Unity-Technologies/ROS-TCP-Endpoint\"\u003eROS-TCP-Endpoint\u003c/a\u003e\npackage into the workspace, build it, and run the endpoint node. On the Unity side, the\n\u003ca href=\"https://github.com/Unity-Technologies/ROS-TCP-Connector\"\u003eROS-TCP-Connector\u003c/a\u003e\nhad to be added to the game.\u003c/li\u003e\n\u003cli\u003eI also had to generate the joint states message in Unity.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe ROS-TCP plugin allows information to flow both ways, although I was only\nsubscribing to messages from ROS in Unity. It could also be argued that for\nthis simple demo, ROS could be eliminated by reading the wired controller\ninputs directly in Unity.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/hello-robot-vr/2_ros_tcp.png\" alt=\"ROS-Unity communication\"\u003e\u003c/p\u003e\n\u003cp\u003eFor VR support, I used the \u003ca href=\"https://github.com/ValveSoftware/steamvr_unity_plugin\"\u003eSteamVR Unity\nPlugin\u003c/a\u003e.  It was easy to\nuse, I just had to drop the \u003ccode\u003eCameraRig\u003c/code\u003e prefab into the scene.  I also liked\nthat with Steam VR the same game could, in principle, work with different\nheadsets.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/hello-robot-vr/3_unity_cardboard.png\" alt=\"Unity-cardboard communication\"\u003e\u003c/p\u003e\n\u003ch3\u003eCardboard viewer\u003c/h3\u003e\n\u003cp\u003eI was using a Google Cardboard viewer with a Moto X4 Android phone. Connecting\nthe Unity game to the smartphone was cumbersome and the least disciplined\ncomponent in my system.\u003c/p\u003e\n\u003cp\u003eI used \u003ca href=\"https://www.trinusvirtualreality.com/trinus-cardboard/\"\u003eTrinus Cardboard\nVR\u003c/a\u003e to display the\nUnity game on the smartphone. On the Android end, I downloaded the Trinus CBVR\nLite app.  On the PC end, I had so far been working on an Ubuntu 20.04 host.\nThe Unity Hub and Editor worked well for me on Ubuntu. But the provided Trinus\nCardboard VR PC server was for Windows. I did try the open source\n\u003ca href=\"https://github.com/MyrikLD/LinusTrinus\"\u003eLinusTrinus\u003c/a\u003e for running a Trinus\nserver in Linux, but it didn't work for me. I was forced to move to a Windows\nhost.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/hello-robot-vr/4_unity_trinus_cardboard.png\" alt=\"Unity-cardboard communication using Trinus\"\u003e\u003c/p\u003e\n\u003cp\u003eUsing the Unity Editor on Ubuntu, I built the Unity game to run on Windows. I\ninstalled Docker on Windows and re-created the ROS image there. But I hit\nanother roadblock when trying to access the wired controller in the ROS\ncontainer. This was a \u003ca href=\"https://github.com/microsoft/WSL/issues/2195\"\u003eknown\nissue\u003c/a\u003e on Windows. Instead of\ninvestigating workarounds, I switched to running the ROS nodes in an Ubuntu VM\nusing \u003ca href=\"https://www.virtualbox.org/\"\u003eVirtualBox\u003c/a\u003e. This was an unsatisfying\nchange, but I was close to the end. The VM ran reliably after a small amount of\nconfiguration to pass-through the Xbox controller, and setup network so that\nthe ROS TCP node could communicate with the Unity game.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/hello-robot-vr/5_overall.png\" alt=\"overall system\"\u003e\u003c/p\u003e\n\u003ch2\u003eResult\u003c/h2\u003e\n\u003cp\u003eOnce all components were setup and running, I could move the robot with the\ncontroller, and view the results on my smartphone and the Cardboard.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/hello-robot-vr/recording.gif\" alt=\"recording\"\u003e\u003c/p\u003e\n\u003cp\u003eAn aspect that could have used more cleanup was the launch of components. I\nended up with a fairly long list of steps to follow to run the demo, starting\nwith having the controlled plugged in, running processes in a specific order,\nchecking various IPs to ensure that comms worked, etc.\u003c/p\u003e\n\u003ch2\u003eSteps that didn't work\u003c/h2\u003e\n\u003cp\u003eA list of other things that I tried that were not part of the final system.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eI tried running Unity Editor in a container, just for curiosity. I couldn't\nget it to work, and most of the guides online\n(\u003ca href=\"https://johnaustin.io/articles/2020/running-unity-20201-in-docker\"\u003eexample\u003c/a\u003e)\nwere about running Unity in a headless mode for CI purposes. I didn't have prior\nexperience with Unity and wanted to try out the Editor, so continued with a\nhost install.\u003c/li\u003e\n\u003cli\u003eI thought about running the Unity game on the phone. I was able to run the\ndemo game from the Cardboard unity plugin\n\u003ca href=\"https://developers.google.com/cardboard/develop/unity/quickstart\"\u003equickstart\u003c/a\u003e.\nBut Steam VR seemed like a better option.\u003c/li\u003e\n\u003cli\u003eI could not get the wired Xbox controller to work if plugged into the\nsmartphone, e.g. to play an Android game with controller support. In\naddition, Unity \u003ca href=\"https://docs.unity3d.com/Packages/com.unity.inputsystem@1.0/manual/SupportedDevices.html\"\u003edid not\nsupport\u003c/a\u003e\nthe Xbox 360 controller on Android.\u003c/li\u003e\n\u003cli\u003eI could not get \u003ca href=\"https://docs.unity3d.com/2020.3/Documentation/Manual/UnityRemote5.html\"\u003eUnity\nRemote\u003c/a\u003e\nto work when running the Unity Editor in Ubuntu. It may have had something to\ndo with \u003ccode\u003eadb\u003c/code\u003e drivers.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eFinal thoughts\u003c/h2\u003e\n\u003cp\u003eIf I had time to work on one additional feature, it would be some form of\ncontrol by passing information back along VR -\u003e Unity -\u003e ROS. I'd have to think\nof a convincing yet simple demo, e.g. using gesture to specify a desired\nend-effector position, then planning and executing a motion. Effective control\nwith a gamepad and RViz already requires some thought, as in this\n\u003ca href=\"https://www.youtube.com/watch?v=p_x-HRagLpo\"\u003evideo\u003c/a\u003e that\n\u003ca href=\"https://ros-planning.github.io/moveit_tutorials/doc/joystick_control_teleoperation/joystick_control_teleoperation_tutorial.html\"\u003edemos\u003c/a\u003e\njoystick control in MoveIt.\u003c/p\u003e\n"])</script><script>self.__next_f.push([1,"5:[\"$\",\"article\",null,{\"className\":\"post\",\"children\":[[\"$\",\"h1\",null,{\"children\":\"Hello Robot in VR\"}],[\"$\",\"p\",null,{\"children\":\"2021-12-19\"}],[\"$\",\"div\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"$c\"}}]]}]\na:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"1\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"2\",{\"children\":\"Hello Robot in VR\"}],[\"$\",\"meta\",\"3\",{\"name\":\"description\",\"content\":\"Notes of an armchair roboticist\"}]]\n4:null\n"])</script><script>self.__next_f.push([1,""])</script></body></html>