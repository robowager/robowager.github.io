<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="stylesheet" href="/_next/static/css/0d132565a6ef74ad.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-a675ae6e161078f1.js"/><script src="/_next/static/chunks/fd9d1056-749e5812300142af.js" async=""></script><script src="/_next/static/chunks/117-36479844a8b7f1e4.js" async=""></script><script src="/_next/static/chunks/main-app-753b7dd70384852c.js" async=""></script><script src="/_next/static/chunks/972-fe008c56cc430895.js" async=""></script><script src="/_next/static/chunks/app/page-f399e1950cf84cbb.js" async=""></script><title>Small robotics software tips</title><meta name="description" content="Notes of an armchair roboticist"/><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body><article class="post"><h1>Small robotics software tips</h1><p>2024-08-11</p><div><p>Some small tips for writing software for robotics. These are collected from
personal experience, but I don't take credit for them. I learned by working with
engineers better than myself. When I need an example application, I will
consider software for an aerial drone that performs deliveries.</p>
<h1>Contents</h1>
<ul>
<li><a href="#Simulation">Simulation</a>
<ul>
<li><a href="#simulation-levels">Simulation levels</a></li>
<li><a href="#against-simulation">Against simulation</a></li>
</ul>
</li>
<li><a href="#sensors">Sensors</a>
<ul>
<li><a href="#sensor-data">Sensor data</a></li>
<li><a href="#sensor-software-layers">Sensor software layers</a></li>
</ul>
</li>
<li><a href="#comms">Comms</a></li>
<li><a href="#retry-loop">Retry loop</a></li>
<li><a href="#analysis">Analysis</a></li>
</ul>
<h1>Simulation</h1>
<h2>Simulation levels</h2>
<p>Simulation (sim) can be implemented at different levels in the software
stack. For example, the drone system might consist of a navigation process</p>
<pre><code># runs in one process
class Navigation:
    def __init__(self, server):
        """
        :param server: used to receive requests for navigation information
        """
</code></pre>
<p>and a separate mission execution process.</p>
<pre><code># runs in another process
class MissionExecutor:
    def __init__(self, navigation_client):
        """
        :param navigation_client: used to query the navigation server
        """
        self._navigation_client = navigation_client
</code></pre>
<p>When executing a trajectory, the mission executor might query navigation for the
next waypoint to go to.</p>
<pre><code>class MissionExecutor:
    def execute_trajectory(self):
        while continue_execution:
            next_waypoint = self._navigation_client.get_next_waypoint()
</code></pre>
<p>We have two options for sim, depending on the motivation.</p>
<ul>
<li>We can run the <code>navigation_client</code> in a sim mode, in which it makes no server
requests and directly returns a waypoint. The advantage is simpler setup, as
no navigation process is needed. Being able to run software easily is a
motivation of simulation.</li>
<li>We can run the navigation process in a sim mode. The advantage is a more
representative setup, as the mission executor uses the real
<code>navigation_client</code>. Being able to run software realistically is another
motivation.</li>
</ul>
<p>It is worth keeping in mind is that simulation can be another interface that
needs to be maintained. When a refactor is made, e.g. a new argument added to
<code>get_next_waypoint()</code>, we shouldn't break sim.</p>
<p>Simulation can also be implemented for different components, allowing the system
to be run in a mixed mode, such as simulated perception with a real system. As a
use case, suppose we are evaluating new rotors by executing various trajectories
on a drone. We don't want to risk collisions, so we might run in a wide open
test field that is known to be free of obstacles. We don't care about perception
in this case, and can use simulated perception.</p>
<p>Supporting such mixed sim modes can add configuration complexity. For example,
we might have the following sequence of flags. Any earlier flag specified as
<code>True</code> in configuration should force subsequent ones to be <code>True</code>, regardless of
their specified values:</p>
<ul>
<li><code>simulated</code>: the entire system is simulated,</li>
<li><code>simulated_perception</code>: all of perception is simulated,</li>
<li><code>simulated_point_cloud</code>: the point cloud sensor is simulated.</li>
</ul>
<h2>Against simulation</h2>
<p>Enough virtues of sim have been sung, from enabling development when hardware is
scarce, to generating synthetic data for machine learning. I will state some
vices.</p>
<p>Simulation can be a distraction. Engineers can spend many hours making
simulation itself more featureful, while doing little to improve the performance
of actual software. As an example, for robotics software that consists of many
components, ensuring that their simulated behavior is consistent is a
challenge. To be more concrete, suppose that a difficult operating condition for
the drone is a sandstorm: winds buffet the drone, showing up as disturbances in
inertial sensors, and dust obscures perception, showing up as noise in vision
sensors.</p>
<p>How would this be simulated? If our simulation approach is to immerse the entire
software in a simulated environment, then some engineer is going to work on
building a sandstorm sim. If instead our approach is to sim various components
independently, we need to somehow share sim state, such that the inertial and
vision sensors' output is correlated.</p>
<p>How about not simulating? A heuristic is: if hardware is idle, an engineer
should not be working on sim. I concede that various issues crop up as soon as
one starts working with hardware. You might end up debugging device issues
rather than working on a fancy algorithm. But you might also come up with a
script to check that all devices are up and running. It could become a utility
that helps everyone run a real system, which is part of what robotics is about.</p>
<p>Robotics software builds a model of the world (whether implicit or
explicit). This model should be tested against reality, not simulation. I have
found unit tests with real data a useful alternative to simulation. Where I have
simulated a component, it could have equivalently been called a mock or no-op
component, as in the example above of testing trajectories on a real system
without perception. Mock components are also useful for integration tests, where
often deterministic behavior is needed, not simulated noise.</p>
<h1>Sensors</h1>
<h2>Sensor data</h2>
<p>As an example for this section, assume that our drone carries a payload in a
compartment. The payload is delivered at the destination by opening a hatch. A
displacement sensor in the compartment provides a binary signal of 0 when the
compartment is empty, and 1 otherwise. Nominal use of the sensor is that it
should read</p>
<ul>
<li>1 before delivery, as a check that the payload has not been unintentionally
dropped,</li>
<li>0 after delivery, as a check that the payload is not stuck after commanding
the hatch to open.</li>
</ul>
<p><img src="/small-robotics-sw-tips/displacement_sensor.png" alt="displacement sensor"></p>
<p>Using sensors starts with calibration, which can take significant work for
complex sensors. Even the simple displacement sensor likely needs calibration
for how much displacement changes its value go from 0 to 1.</p>
<p>Working with time-varying data can be tricky. A direct implementation of the
nominal use of the displacement sensor will likely be insufficient. As the drone
moves about, the payload might jitter in the compartment, causing blips in the
sensor reading. The data might need to be smoothed before use.</p>
<p>The environment can be a factor. Consider a drone following a straight line in a
calm vs windy sky. With wind, the drone has to constantly correct for
disturbances, and the executed path might look like it oscillates around the
straight line. The displacement sensor will have more blips in this situation.</p>
<p>Making decisions for a robotics task can involve understanding the interaction
of sensing and actuation (or perception and planning). One way to deliver the
payload is for the drone to gently hover over the destination and open the
hatch. The displacement sensor reading might go from 1 to 0 as expected. But
suppose we implement a more aggressive delivery maneuver in order to speed up
missions.</p>
<pre><code>def aggressive_delivery():
    dip_towards_destination()
    open_hatch_mid_flight()
    sharply_ascend()
</code></pre>
<p>The trajectory and sensor readings are depicted in the image below. Before
delivery, at time <code>t0</code>, the sensor reads 1. After delivery, at time <code>t1</code>, we
might expect the sensor to read 0. But a large vertical acceleration during the
ascent, ongoing at time <code>t2</code>, can compress the sensor, causing a steady reading
of 1 for a while. This is independent of whether the compartment is empty, and
might cause us to falsely conclude that the payload drop failed.</p>
<p><img src="/small-robotics-sw-tips/aggressive_delivery.png" alt="aggressive delivery"></p>
<p>An option is to read the displacement sensor a fixed delay after calling
<code>open_hatch_mid_flight</code>. But a tip is that such delays can be unreliable when
working with sensor data. Instead, prefer using actuation information (or more
broadly, knowledge of the robot's actions). From the planned ascent trajectory,
we should be able to figure out a waypoint at which the vertical acceleration
reduces, time <code>t3</code> in the image. This gives us a better starting point of when
to read the sensor. Logging raw sensor data, along with when the sensor is read,
can be very helpful in tuning such logic.</p>
<h2>Sensor software layers</h2>
<p>A way to structure software around a sensor is by</p>
<ul>
<li>driver,</li>
<li>comms,</li>
<li>application.</li>
</ul>
<p>For the binary displacement sensor example, the driver can implement the feature
of smoothing data.</p>
<pre><code>class DisplacementSensor:
    def get_raw_data(self):
        ...

    def get_smoothed_data(self):
        ...
</code></pre>
<p>Python's context managers can be put to good use when working with sensors, for
similar reasons as resources like file handles. The context manager can check
that the sensor is in a good state on entry, and perform cleanup and error
handling on exit.</p>
<p>A more complex sensor like a camera might have a comms layer to serve data
requests. The separation of driver and comms software is useful to allow
multiple comms implementations, such as using ROS or protocol buffers. The
separation also makes it easier to test the driver layer, by not having to setup
comms in a unit test.</p>
<p>The application layer can contain task-specific functionality. For the drone
software this could be</p>
<pre><code>class PayloadSensor:
    def __init__(self, sensor):
        """
        :param sensor: the underlying sensor, could
            be the driver or a comms client
        """

    def check_delivery(self, trajectory, execution_progress):
        """
        :param trajectory: the planned aggressive delivery trajectory, used
            to determine when to read the sensor
        :param execution_progress: some sort of feedback that tells us
            when the hatch was opened, the current waypoint in the delivery
            trajectory, etc
        """
</code></pre>
<p>The application layer makes the task requirements clear. Suppose we wanted to
upgrade from a binary to continuous displacement sensor, a good goal is to try
using the same <code>PayloadSensor</code>.</p>
<p>Conversely, we might find a separate use for the binary displacement
sensor. Suppose the docking platform runs its own software. We can add a
displacement sensor to the platform to confirm that a drone successfully lands
on it. The sensor readings might be more well-behaved here. We can use the same
driver and comms layer, but skip the more complicated <code>PayloadSensor</code>.</p>
<p>For a robot system, there might be many modes of running the same sensor, which
have to be supported via configuration.</p>
<ul>
<li>Not using the sensor, when no configuration is specified. This might be the
choice if the hardware was initially designed without the sensor.</li>
<li>Creating a sensor interface, if specified in configuration.</li>
<li>Disabling the sensor, if explicitly configured. This is useful if the sensor
breaks during operation.</li>
<li>Running the sensor in shadow mode, where we collect data in the background,
but don't make decisions on it.</li>
</ul>
<h1>Comms</h1>
<p>Dealing with comms is essential in robotics software, where there are many
processes and devices. Here are some tips for dealing with comms-related issues,
referring to the earlier example of the drone having a separate navigation and
mission executor process.</p>
<p>A client needs to check that a network connection to the server exists. Instead
of performing such a check on startup, prefer delaying it to when the client
makes a request. This removes assumptions on process ordering during startup.</p>
<p>Build in robustness due to network lags. Suppose the mission executor expects
waypoints at a certain rate. What if the next waypoint is delayed? It might be
alright to stall for as long as possible, so as not to abort the trajectory.</p>
<p>Build in robustness to processes dying and restarting, which is more drastic
than delays. For example, instead of making a separate call to navigation for
each waypoint, the mission executor might receive waypoints as part of one
long-lived network call (such as a ROS action or streaming RPC).</p>
<ul>
<li>What if the navigation process dies in the middle of this call? Can the
mission executor detect and recover from it?</li>
<li>What if the navigation process restarts? Will it pick up state from the
previous call, or should the mission executor start a new one?</li>
</ul>
<p>Another heuristic: for every server introduced, there should be a corresponding
console. Here's what I mean. Suppose the drone delivery system has the following
processes:</p>
<ul>
<li>scheduler,</li>
<li>mission executor,</li>
<li>navigation,</li>
<li>sensor.</li>
</ul>
<p>Every process exposes a server, and has a client to the process below. Every
process receives requests from the process above, and makes requests of the
process below. Then for each server, there should exist a console, which is
simply a lightweight client that can be run manually in a terminal.</p>
<p>The main purpose of a console is manual debugging. Suppose we notice a sensor
issue occur during a rare mission. Instead of waiting for the scheduler to
assign such a mission again, we could fire up a mission console, and submit it
to the mission executor. Or a perception engineer might open up a sensor console
to directly request data from the sensor.</p>
<p>Ideally, a console needs little new code. It should be designed to be
user-friendly, e.g. if a request message has a large number of fields, defaults
can be supplied.</p>
<h1>Retry loop</h1>
<p>The retry loop is a pattern to make software robust.</p>
<pre><code>def retry_loop():
    while continue_attempts:
        try:
            task()
        except Exception as error:
            handle_exception(error)

    if task_failed():
        fallback()
</code></pre>
<p>Exceptions encountered while trying to run a task are fed to a handler, an
example of catching exceptions high up in the call stack. We continue trying
till the task succeeds, or too many exceptions occur. If the task failed, we
might execute a fallback. A classic example of a fallback with mobile robots is
coming to a safe stop.</p>
<p>Retry behavior can be configurable. For example, we might have retry limits for
different exceptions. These might be adjusted in both directions.</p>
<ul>
<li>If a network is known to be flaky, we can increase the limit for connection
errors.</li>
<li>If we determine that a sensor never recovers from a kind of malfunction, we
can decrease the number of retries on that error to 0.</li>
</ul>
<p>A fine point is that the retry loop assumes that a single exception occurs
during an attempt, but this may be false. Suppose our drone has multiple rotors,
and one of them faults while executing a trajectory. It could raise two errors:
a hardware error from the rotor driver, and a trajectory error that we failed to
reach a waypoint. The root cause for the errors are the same, but their handling
will be different (e.g. restarting the rotor, and re-executing a trajectory with
the remaining rotors).</p>
<p>The retry loop can be applied to different levels in the software stack. For
example</p>
<ul>
<li>a driver could retry querying a sensor,</li>
<li>a comms layer could retry on connection errors,</li>
<li>mission execution can retry high-level actions.</li>
</ul>
<p>Advantages of multiple retry loops are that</p>
<ul>
<li>exceptions are retried on by components that can handle them best,</li>
<li>only sufficiently severe exceptions are passed on to higher levels.</li>
</ul>
<p>Watch out for these bad uses of retry loops which introduce inefficiencies.</p>
<ul>
<li>The retry loop can hide errors that are fixable. An example is that first-time
query of a sensor might always fail due to a bug with its initialization. We
might miss the bug if we are monitoring only task success, and not the number
of retries it takes to succeed.</li>
<li>Multiple levels of retry loops might retry on the same exception, effectively
increasing the retry limit. This is confusing code and it is best to limit
handling a particular exception to a single loop.</li>
</ul>
<h1>Analysis</h1>
<p>Adding features is only one part of robotics software. As important is analysis
of performance once software is deployed. Robots generate a lot of data related
to sensors, actuators, and decision-making. A tip is to invest effort in making
both data and analysis widely accessible.</p>
<p>Making data accessible can look like uploading data to a shared location, rather
than locked up on a robot computer. Making analysis accessible might look like
committing jupyter notebooks to a repository. Analysis scripts don't have to be
as clean and general as production code. They only need to be clear enough for
others to use as a starting point.</p>
<p>An analysis exercise I have often gone through is imposing of order on a mass of
anomalous events, like so.</p>
<ul>
<li>Collect all cases of failure to complete a mission.</li>
<li>Start manual investigation, annotating with human-readable reasons.</li>
<li>As categories start to emerge, name them with fixed labels,
e.g. <code>NO_WAYPOINTS_FOUND</code>, <code>PAYLOAD_DROP_FAILED</code>.</li>
<li>Enter the labels in code, say as an enum. This is the point of crossing over
from informal analysis of reasons to formal labels in software.</li>
<li>Start logging the labels along with live missions. This creates a partially
annotated dataset that can further be used, e.g. to tune retry limits, or
train a new machine learning model for navigation.</li>
</ul>
</div></article><footer><a href="/">Home</a></footer><script src="/_next/static/chunks/webpack-a675ae6e161078f1.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script>self.__next_f.push([1,"1:HL[\"/_next/static/css/0d132565a6ef74ad.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"2:I[2846,[],\"\"]\n5:I[4707,[],\"\"]\n7:I[6423,[],\"\"]\n8:I[2972,[\"972\",\"static/chunks/972-fe008c56cc430895.js\",\"931\",\"static/chunks/app/page-f399e1950cf84cbb.js\"],\"\"]\na:I[1060,[],\"\"]\n6:[\"slug\",\"small-robotics-sw-tips\",\"d\"]\nb:[]\n"])</script><script>self.__next_f.push([1,"0:[\"$\",\"$L2\",null,{\"buildId\":\"j5tnp2GFEM0E6s9J32yOg\",\"assetPrefix\":\"\",\"urlParts\":[\"\",\"posts\",\"small-robotics-sw-tips\"],\"initialTree\":[\"\",{\"children\":[\"posts\",{\"children\":[[\"slug\",\"small-robotics-sw-tips\",\"d\"],{\"children\":[\"__PAGE__?{\\\"slug\\\":\\\"small-robotics-sw-tips\\\"}\",{}]}]}]},\"$undefined\",\"$undefined\",true],\"initialSeedData\":[\"\",{\"children\":[\"posts\",{\"children\":[[\"slug\",\"small-robotics-sw-tips\",\"d\"],{\"children\":[\"__PAGE__\",{},[[\"$L3\",\"$L4\",null],null],null]},[null,[\"$\",\"$L5\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"posts\",\"children\",\"$6\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L7\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\"}]],null]},[null,[\"$\",\"$L5\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"posts\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L7\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\"}]],null]},[[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/0d132565a6ef74ad.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"body\",null,{\"children\":[[\"$\",\"$L5\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L7\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":\"404\"}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],\"notFoundStyles\":[]}],[\"$\",\"footer\",null,{\"children\":[\"$\",\"$L8\",null,{\"href\":\"/\",\"children\":\"Home\"}]}]]}]}]],null],null],\"couldBeIntercepted\":false,\"initialHead\":[null,\"$L9\"],\"globalErrorComponent\":\"$a\",\"missingSlots\":\"$Wb\"}]\n"])</script><script>self.__next_f.push([1,"c:T479f,"])</script><script>self.__next_f.push([1,"\u003cp\u003eSome small tips for writing software for robotics. These are collected from\npersonal experience, but I don't take credit for them. I learned by working with\nengineers better than myself. When I need an example application, I will\nconsider software for an aerial drone that performs deliveries.\u003c/p\u003e\n\u003ch1\u003eContents\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#Simulation\"\u003eSimulation\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#simulation-levels\"\u003eSimulation levels\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#against-simulation\"\u003eAgainst simulation\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#sensors\"\u003eSensors\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#sensor-data\"\u003eSensor data\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#sensor-software-layers\"\u003eSensor software layers\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#comms\"\u003eComms\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#retry-loop\"\u003eRetry loop\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#analysis\"\u003eAnalysis\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003eSimulation\u003c/h1\u003e\n\u003ch2\u003eSimulation levels\u003c/h2\u003e\n\u003cp\u003eSimulation (sim) can be implemented at different levels in the software\nstack. For example, the drone system might consist of a navigation process\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e# runs in one process\nclass Navigation:\n    def __init__(self, server):\n        \"\"\"\n        :param server: used to receive requests for navigation information\n        \"\"\"\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eand a separate mission execution process.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e# runs in another process\nclass MissionExecutor:\n    def __init__(self, navigation_client):\n        \"\"\"\n        :param navigation_client: used to query the navigation server\n        \"\"\"\n        self._navigation_client = navigation_client\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWhen executing a trajectory, the mission executor might query navigation for the\nnext waypoint to go to.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eclass MissionExecutor:\n    def execute_trajectory(self):\n        while continue_execution:\n            next_waypoint = self._navigation_client.get_next_waypoint()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWe have two options for sim, depending on the motivation.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eWe can run the \u003ccode\u003enavigation_client\u003c/code\u003e in a sim mode, in which it makes no server\nrequests and directly returns a waypoint. The advantage is simpler setup, as\nno navigation process is needed. Being able to run software easily is a\nmotivation of simulation.\u003c/li\u003e\n\u003cli\u003eWe can run the navigation process in a sim mode. The advantage is a more\nrepresentative setup, as the mission executor uses the real\n\u003ccode\u003enavigation_client\u003c/code\u003e. Being able to run software realistically is another\nmotivation.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIt is worth keeping in mind is that simulation can be another interface that\nneeds to be maintained. When a refactor is made, e.g. a new argument added to\n\u003ccode\u003eget_next_waypoint()\u003c/code\u003e, we shouldn't break sim.\u003c/p\u003e\n\u003cp\u003eSimulation can also be implemented for different components, allowing the system\nto be run in a mixed mode, such as simulated perception with a real system. As a\nuse case, suppose we are evaluating new rotors by executing various trajectories\non a drone. We don't want to risk collisions, so we might run in a wide open\ntest field that is known to be free of obstacles. We don't care about perception\nin this case, and can use simulated perception.\u003c/p\u003e\n\u003cp\u003eSupporting such mixed sim modes can add configuration complexity. For example,\nwe might have the following sequence of flags. Any earlier flag specified as\n\u003ccode\u003eTrue\u003c/code\u003e in configuration should force subsequent ones to be \u003ccode\u003eTrue\u003c/code\u003e, regardless of\ntheir specified values:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003esimulated\u003c/code\u003e: the entire system is simulated,\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003esimulated_perception\u003c/code\u003e: all of perception is simulated,\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003esimulated_point_cloud\u003c/code\u003e: the point cloud sensor is simulated.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eAgainst simulation\u003c/h2\u003e\n\u003cp\u003eEnough virtues of sim have been sung, from enabling development when hardware is\nscarce, to generating synthetic data for machine learning. I will state some\nvices.\u003c/p\u003e\n\u003cp\u003eSimulation can be a distraction. Engineers can spend many hours making\nsimulation itself more featureful, while doing little to improve the performance\nof actual software. As an example, for robotics software that consists of many\ncomponents, ensuring that their simulated behavior is consistent is a\nchallenge. To be more concrete, suppose that a difficult operating condition for\nthe drone is a sandstorm: winds buffet the drone, showing up as disturbances in\ninertial sensors, and dust obscures perception, showing up as noise in vision\nsensors.\u003c/p\u003e\n\u003cp\u003eHow would this be simulated? If our simulation approach is to immerse the entire\nsoftware in a simulated environment, then some engineer is going to work on\nbuilding a sandstorm sim. If instead our approach is to sim various components\nindependently, we need to somehow share sim state, such that the inertial and\nvision sensors' output is correlated.\u003c/p\u003e\n\u003cp\u003eHow about not simulating? A heuristic is: if hardware is idle, an engineer\nshould not be working on sim. I concede that various issues crop up as soon as\none starts working with hardware. You might end up debugging device issues\nrather than working on a fancy algorithm. But you might also come up with a\nscript to check that all devices are up and running. It could become a utility\nthat helps everyone run a real system, which is part of what robotics is about.\u003c/p\u003e\n\u003cp\u003eRobotics software builds a model of the world (whether implicit or\nexplicit). This model should be tested against reality, not simulation. I have\nfound unit tests with real data a useful alternative to simulation. Where I have\nsimulated a component, it could have equivalently been called a mock or no-op\ncomponent, as in the example above of testing trajectories on a real system\nwithout perception. Mock components are also useful for integration tests, where\noften deterministic behavior is needed, not simulated noise.\u003c/p\u003e\n\u003ch1\u003eSensors\u003c/h1\u003e\n\u003ch2\u003eSensor data\u003c/h2\u003e\n\u003cp\u003eAs an example for this section, assume that our drone carries a payload in a\ncompartment. The payload is delivered at the destination by opening a hatch. A\ndisplacement sensor in the compartment provides a binary signal of 0 when the\ncompartment is empty, and 1 otherwise. Nominal use of the sensor is that it\nshould read\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e1 before delivery, as a check that the payload has not been unintentionally\ndropped,\u003c/li\u003e\n\u003cli\u003e0 after delivery, as a check that the payload is not stuck after commanding\nthe hatch to open.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/small-robotics-sw-tips/displacement_sensor.png\" alt=\"displacement sensor\"\u003e\u003c/p\u003e\n\u003cp\u003eUsing sensors starts with calibration, which can take significant work for\ncomplex sensors. Even the simple displacement sensor likely needs calibration\nfor how much displacement changes its value go from 0 to 1.\u003c/p\u003e\n\u003cp\u003eWorking with time-varying data can be tricky. A direct implementation of the\nnominal use of the displacement sensor will likely be insufficient. As the drone\nmoves about, the payload might jitter in the compartment, causing blips in the\nsensor reading. The data might need to be smoothed before use.\u003c/p\u003e\n\u003cp\u003eThe environment can be a factor. Consider a drone following a straight line in a\ncalm vs windy sky. With wind, the drone has to constantly correct for\ndisturbances, and the executed path might look like it oscillates around the\nstraight line. The displacement sensor will have more blips in this situation.\u003c/p\u003e\n\u003cp\u003eMaking decisions for a robotics task can involve understanding the interaction\nof sensing and actuation (or perception and planning). One way to deliver the\npayload is for the drone to gently hover over the destination and open the\nhatch. The displacement sensor reading might go from 1 to 0 as expected. But\nsuppose we implement a more aggressive delivery maneuver in order to speed up\nmissions.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edef aggressive_delivery():\n    dip_towards_destination()\n    open_hatch_mid_flight()\n    sharply_ascend()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe trajectory and sensor readings are depicted in the image below. Before\ndelivery, at time \u003ccode\u003et0\u003c/code\u003e, the sensor reads 1. After delivery, at time \u003ccode\u003et1\u003c/code\u003e, we\nmight expect the sensor to read 0. But a large vertical acceleration during the\nascent, ongoing at time \u003ccode\u003et2\u003c/code\u003e, can compress the sensor, causing a steady reading\nof 1 for a while. This is independent of whether the compartment is empty, and\nmight cause us to falsely conclude that the payload drop failed.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/small-robotics-sw-tips/aggressive_delivery.png\" alt=\"aggressive delivery\"\u003e\u003c/p\u003e\n\u003cp\u003eAn option is to read the displacement sensor a fixed delay after calling\n\u003ccode\u003eopen_hatch_mid_flight\u003c/code\u003e. But a tip is that such delays can be unreliable when\nworking with sensor data. Instead, prefer using actuation information (or more\nbroadly, knowledge of the robot's actions). From the planned ascent trajectory,\nwe should be able to figure out a waypoint at which the vertical acceleration\nreduces, time \u003ccode\u003et3\u003c/code\u003e in the image. This gives us a better starting point of when\nto read the sensor. Logging raw sensor data, along with when the sensor is read,\ncan be very helpful in tuning such logic.\u003c/p\u003e\n\u003ch2\u003eSensor software layers\u003c/h2\u003e\n\u003cp\u003eA way to structure software around a sensor is by\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003edriver,\u003c/li\u003e\n\u003cli\u003ecomms,\u003c/li\u003e\n\u003cli\u003eapplication.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFor the binary displacement sensor example, the driver can implement the feature\nof smoothing data.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eclass DisplacementSensor:\n    def get_raw_data(self):\n        ...\n\n    def get_smoothed_data(self):\n        ...\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ePython's context managers can be put to good use when working with sensors, for\nsimilar reasons as resources like file handles. The context manager can check\nthat the sensor is in a good state on entry, and perform cleanup and error\nhandling on exit.\u003c/p\u003e\n\u003cp\u003eA more complex sensor like a camera might have a comms layer to serve data\nrequests. The separation of driver and comms software is useful to allow\nmultiple comms implementations, such as using ROS or protocol buffers. The\nseparation also makes it easier to test the driver layer, by not having to setup\ncomms in a unit test.\u003c/p\u003e\n\u003cp\u003eThe application layer can contain task-specific functionality. For the drone\nsoftware this could be\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eclass PayloadSensor:\n    def __init__(self, sensor):\n        \"\"\"\n        :param sensor: the underlying sensor, could\n            be the driver or a comms client\n        \"\"\"\n\n    def check_delivery(self, trajectory, execution_progress):\n        \"\"\"\n        :param trajectory: the planned aggressive delivery trajectory, used\n            to determine when to read the sensor\n        :param execution_progress: some sort of feedback that tells us\n            when the hatch was opened, the current waypoint in the delivery\n            trajectory, etc\n        \"\"\"\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe application layer makes the task requirements clear. Suppose we wanted to\nupgrade from a binary to continuous displacement sensor, a good goal is to try\nusing the same \u003ccode\u003ePayloadSensor\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eConversely, we might find a separate use for the binary displacement\nsensor. Suppose the docking platform runs its own software. We can add a\ndisplacement sensor to the platform to confirm that a drone successfully lands\non it. The sensor readings might be more well-behaved here. We can use the same\ndriver and comms layer, but skip the more complicated \u003ccode\u003ePayloadSensor\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eFor a robot system, there might be many modes of running the same sensor, which\nhave to be supported via configuration.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eNot using the sensor, when no configuration is specified. This might be the\nchoice if the hardware was initially designed without the sensor.\u003c/li\u003e\n\u003cli\u003eCreating a sensor interface, if specified in configuration.\u003c/li\u003e\n\u003cli\u003eDisabling the sensor, if explicitly configured. This is useful if the sensor\nbreaks during operation.\u003c/li\u003e\n\u003cli\u003eRunning the sensor in shadow mode, where we collect data in the background,\nbut don't make decisions on it.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003eComms\u003c/h1\u003e\n\u003cp\u003eDealing with comms is essential in robotics software, where there are many\nprocesses and devices. Here are some tips for dealing with comms-related issues,\nreferring to the earlier example of the drone having a separate navigation and\nmission executor process.\u003c/p\u003e\n\u003cp\u003eA client needs to check that a network connection to the server exists. Instead\nof performing such a check on startup, prefer delaying it to when the client\nmakes a request. This removes assumptions on process ordering during startup.\u003c/p\u003e\n\u003cp\u003eBuild in robustness due to network lags. Suppose the mission executor expects\nwaypoints at a certain rate. What if the next waypoint is delayed? It might be\nalright to stall for as long as possible, so as not to abort the trajectory.\u003c/p\u003e\n\u003cp\u003eBuild in robustness to processes dying and restarting, which is more drastic\nthan delays. For example, instead of making a separate call to navigation for\neach waypoint, the mission executor might receive waypoints as part of one\nlong-lived network call (such as a ROS action or streaming RPC).\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eWhat if the navigation process dies in the middle of this call? Can the\nmission executor detect and recover from it?\u003c/li\u003e\n\u003cli\u003eWhat if the navigation process restarts? Will it pick up state from the\nprevious call, or should the mission executor start a new one?\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAnother heuristic: for every server introduced, there should be a corresponding\nconsole. Here's what I mean. Suppose the drone delivery system has the following\nprocesses:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003escheduler,\u003c/li\u003e\n\u003cli\u003emission executor,\u003c/li\u003e\n\u003cli\u003enavigation,\u003c/li\u003e\n\u003cli\u003esensor.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eEvery process exposes a server, and has a client to the process below. Every\nprocess receives requests from the process above, and makes requests of the\nprocess below. Then for each server, there should exist a console, which is\nsimply a lightweight client that can be run manually in a terminal.\u003c/p\u003e\n\u003cp\u003eThe main purpose of a console is manual debugging. Suppose we notice a sensor\nissue occur during a rare mission. Instead of waiting for the scheduler to\nassign such a mission again, we could fire up a mission console, and submit it\nto the mission executor. Or a perception engineer might open up a sensor console\nto directly request data from the sensor.\u003c/p\u003e\n\u003cp\u003eIdeally, a console needs little new code. It should be designed to be\nuser-friendly, e.g. if a request message has a large number of fields, defaults\ncan be supplied.\u003c/p\u003e\n\u003ch1\u003eRetry loop\u003c/h1\u003e\n\u003cp\u003eThe retry loop is a pattern to make software robust.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edef retry_loop():\n    while continue_attempts:\n        try:\n            task()\n        except Exception as error:\n            handle_exception(error)\n\n    if task_failed():\n        fallback()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eExceptions encountered while trying to run a task are fed to a handler, an\nexample of catching exceptions high up in the call stack. We continue trying\ntill the task succeeds, or too many exceptions occur. If the task failed, we\nmight execute a fallback. A classic example of a fallback with mobile robots is\ncoming to a safe stop.\u003c/p\u003e\n\u003cp\u003eRetry behavior can be configurable. For example, we might have retry limits for\ndifferent exceptions. These might be adjusted in both directions.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIf a network is known to be flaky, we can increase the limit for connection\nerrors.\u003c/li\u003e\n\u003cli\u003eIf we determine that a sensor never recovers from a kind of malfunction, we\ncan decrease the number of retries on that error to 0.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eA fine point is that the retry loop assumes that a single exception occurs\nduring an attempt, but this may be false. Suppose our drone has multiple rotors,\nand one of them faults while executing a trajectory. It could raise two errors:\na hardware error from the rotor driver, and a trajectory error that we failed to\nreach a waypoint. The root cause for the errors are the same, but their handling\nwill be different (e.g. restarting the rotor, and re-executing a trajectory with\nthe remaining rotors).\u003c/p\u003e\n\u003cp\u003eThe retry loop can be applied to different levels in the software stack. For\nexample\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ea driver could retry querying a sensor,\u003c/li\u003e\n\u003cli\u003ea comms layer could retry on connection errors,\u003c/li\u003e\n\u003cli\u003emission execution can retry high-level actions.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAdvantages of multiple retry loops are that\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eexceptions are retried on by components that can handle them best,\u003c/li\u003e\n\u003cli\u003eonly sufficiently severe exceptions are passed on to higher levels.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWatch out for these bad uses of retry loops which introduce inefficiencies.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe retry loop can hide errors that are fixable. An example is that first-time\nquery of a sensor might always fail due to a bug with its initialization. We\nmight miss the bug if we are monitoring only task success, and not the number\nof retries it takes to succeed.\u003c/li\u003e\n\u003cli\u003eMultiple levels of retry loops might retry on the same exception, effectively\nincreasing the retry limit. This is confusing code and it is best to limit\nhandling a particular exception to a single loop.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003eAnalysis\u003c/h1\u003e\n\u003cp\u003eAdding features is only one part of robotics software. As important is analysis\nof performance once software is deployed. Robots generate a lot of data related\nto sensors, actuators, and decision-making. A tip is to invest effort in making\nboth data and analysis widely accessible.\u003c/p\u003e\n\u003cp\u003eMaking data accessible can look like uploading data to a shared location, rather\nthan locked up on a robot computer. Making analysis accessible might look like\ncommitting jupyter notebooks to a repository. Analysis scripts don't have to be\nas clean and general as production code. They only need to be clear enough for\nothers to use as a starting point.\u003c/p\u003e\n\u003cp\u003eAn analysis exercise I have often gone through is imposing of order on a mass of\nanomalous events, like so.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCollect all cases of failure to complete a mission.\u003c/li\u003e\n\u003cli\u003eStart manual investigation, annotating with human-readable reasons.\u003c/li\u003e\n\u003cli\u003eAs categories start to emerge, name them with fixed labels,\ne.g. \u003ccode\u003eNO_WAYPOINTS_FOUND\u003c/code\u003e, \u003ccode\u003ePAYLOAD_DROP_FAILED\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003eEnter the labels in code, say as an enum. This is the point of crossing over\nfrom informal analysis of reasons to formal labels in software.\u003c/li\u003e\n\u003cli\u003eStart logging the labels along with live missions. This creates a partially\nannotated dataset that can further be used, e.g. to tune retry limits, or\ntrain a new machine learning model for navigation.\u003c/li\u003e\n\u003c/ul\u003e\n"])</script><script>self.__next_f.push([1,"4:[\"$\",\"article\",null,{\"className\":\"post\",\"children\":[[\"$\",\"h1\",null,{\"children\":\"Small robotics software tips\"}],[\"$\",\"p\",null,{\"children\":\"2024-08-11\"}],[\"$\",\"div\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"$c\"}}]]}]\n9:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"1\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"2\",{\"children\":\"Small robotics software tips\"}],[\"$\",\"meta\",\"3\",{\"name\":\"description\",\"content\":\"Notes of an armchair roboticist\"}]]\n3:null\n"])</script></body></html>